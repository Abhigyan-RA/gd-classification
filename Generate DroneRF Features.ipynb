{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d41304d1",
   "metadata": {},
   "source": [
    "### Notebook to generate DroneRF Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da586fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from numpy import sum,isrealobj,sqrt\n",
    "from numpy.random import standard_normal\n",
    "from sklearn.model_selection import train_test_split\n",
    "from spafe.features.lfcc import lfcc\n",
    "import spafe.utils.vis as vis\n",
    "from scipy.signal import get_window\n",
    "import scipy.fftpack as fft\n",
    "from scipy import signal\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import date\n",
    "from tqdm import tqdm\n",
    "\n",
    "# from loading_functions import *\n",
    "from file_paths import *\n",
    "from feat_gen_functions import *\n",
    "\n",
    "import importlib\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8705c452",
   "metadata": {},
   "outputs": [],
   "source": [
    "import feat_gen_functions\n",
    "importlib.reload(feat_gen_functions)\n",
    "from feat_gen_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c83636a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ef851f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "\n",
    "# from pyspark.sql.types import *\n",
    "\n",
    "\n",
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"DroneRFDistributedProcessing\") \\\n",
    "#     .master(\"local[*]\") \\\n",
    "#     .config(\"spark.driver.memory\", \"8g\") \\\n",
    "#     .config(\"spark.executor.memory\", \"8g\") \\\n",
    "#     .config(\"spark.memory.fraction\", \"0.8\") \\\n",
    "#     .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "#     .config(\"spark.default.parallelism\", \"100\") \\\n",
    "#     .config(\"spark.driver.maxResultSize\", \"4g\") \\\n",
    "#     .getOrCreate()\n",
    "\n",
    "# # Broadcast frequently used constants to all workers\n",
    "# fs = spark.sparkContext.broadcast(40e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4c62b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dronerf_raw(main_folder, t_seg):\n",
    "    import os\n",
    "    import numpy as np\n",
    "    import time\n",
    "    from datetime import datetime\n",
    "    import pandas as pd\n",
    "    from pyspark.sql.functions import udf, lit, col\n",
    "    from pyspark.sql.types import ArrayType, FloatType, IntegerType, StructType, StructField, StringType, LongType\n",
    "\n",
    "    print(f\"Started processing at {datetime.now().strftime('%H:%M:%S')}\")\n",
    "    start_time_total = time.time()\n",
    "\n",
    "    # Get file lists\n",
    "    high_freq_files = os.listdir(main_folder+'High/')\n",
    "    low_freq_files = os.listdir(main_folder+'Low/')\n",
    "    \n",
    "    high_freq_files.sort()\n",
    "    low_freq_files.sort()\n",
    "    \n",
    "    print(f\"Found {len(high_freq_files)} high frequency files and {len(low_freq_files)} low frequency files\")\n",
    "    \n",
    "    # Define the segment length\n",
    "    len_seg = int((t_seg/1e3) * 40e6)  # 40 MHz\n",
    "    \n",
    "    # Process files in smaller batches to avoid timeout\n",
    "    batch_size = 5  # Process 5 files at a time\n",
    "    total_batches = (len(high_freq_files) + batch_size - 1) // batch_size\n",
    "    \n",
    "    # Arrays to store results\n",
    "    all_features = []\n",
    "    all_y = []\n",
    "    all_y4 = []\n",
    "    all_y10 = []\n",
    "    \n",
    "    # Track processing statistics\n",
    "    timing_stats = []\n",
    "    successful_files = 0\n",
    "    \n",
    "    # Process files in batches\n",
    "    for batch in range(total_batches):\n",
    "        start_idx = batch * batch_size\n",
    "        end_idx = min((batch + 1) * batch_size, len(high_freq_files))\n",
    "        \n",
    "        print(f\"\\nProcessing batch {batch+1}/{total_batches} (files {start_idx+1}-{end_idx})\")\n",
    "        \n",
    "        # Process each file in the batch\n",
    "        for i in range(start_idx, end_idx):\n",
    "            file_start_time = time.time()\n",
    "            \n",
    "            high_file = high_freq_files[i]\n",
    "            low_file = low_freq_files[i]\n",
    "            \n",
    "            try:\n",
    "                # Get file sizes\n",
    "                high_path = main_folder + 'High/' + high_file\n",
    "                low_path = main_folder + 'Low/' + low_file\n",
    "                file_size_high = os.path.getsize(high_path)\n",
    "                file_size_low = os.path.getsize(low_path)\n",
    "                \n",
    "                print(f\"Processing file {i+1}/{len(high_freq_files)}: {high_file} ({file_size_high/1024:.1f}KB) & {low_file} ({file_size_low/1024:.1f}KB)\")\n",
    "                \n",
    "                # Load RF data\n",
    "                load_start = time.time()\n",
    "                rf_data_h = pd.read_csv(high_path, header=None).values.flatten()\n",
    "                rf_data_l = pd.read_csv(low_path, header=None).values.flatten()\n",
    "                load_time = time.time() - load_start\n",
    "                print(f\"  - Load time: {load_time:.2f}s\")\n",
    "                \n",
    "                if len(rf_data_h) != len(rf_data_l):\n",
    "                    print(f'  - Different lengths: {i}, file name: {low_file}')\n",
    "                    continue\n",
    "                    \n",
    "                if int(high_file[:5]) != int(low_file[:5]):\n",
    "                    print(f\"  - File labels do not match: {high_file} vs {low_file}\")\n",
    "                    continue\n",
    "                    \n",
    "                # Stack features\n",
    "                stack_start = time.time()\n",
    "                rf_sig = np.vstack((rf_data_h, rf_data_l))\n",
    "                stack_time = time.time() - stack_start\n",
    "                \n",
    "                # Calculate segments\n",
    "                n_segs = len(rf_data_h) // len_seg\n",
    "                n_keep = n_segs * len_seg\n",
    "                \n",
    "                if n_segs == 0:\n",
    "                    print(\"  - No segments created (segment length too large)\")\n",
    "                    continue\n",
    "                    \n",
    "                # Split the data\n",
    "                split_start = time.time()\n",
    "                rf_sig_segments = np.split(rf_sig[:, :n_keep], n_segs, axis=1)\n",
    "                split_time = time.time() - split_start\n",
    "                \n",
    "                # Create labels\n",
    "                y_rep = [int(low_file[0])] * n_segs\n",
    "                y4_rep = [int(low_file[:3])] * n_segs\n",
    "                y10_rep = [int(low_file[:5])] * n_segs\n",
    "                \n",
    "                # Add segments to our result arrays\n",
    "                all_features.extend(rf_sig_segments)\n",
    "                all_y.extend(y_rep)\n",
    "                all_y4.extend(y4_rep)\n",
    "                all_y10.extend(y10_rep)\n",
    "                \n",
    "                successful_files += 1\n",
    "                \n",
    "                file_time = time.time() - file_start_time\n",
    "                print(f\"  - Processed {n_segs} segments: Stack time: {stack_time:.2f}s, \"\n",
    "                      f\"Split time: {split_time:.2f}s, Total time: {file_time:.2f}s\")\n",
    "                \n",
    "                # Store timing stats\n",
    "                timing_stats.append({\n",
    "                    \"high_file\": high_file,\n",
    "                    \"low_file\": low_file,\n",
    "                    \"processing_time\": int(file_time * 1000),\n",
    "                    \"file_size_high\": file_size_high,\n",
    "                    \"file_size_low\": file_size_low,\n",
    "                    \"is_valid\": 1\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                file_time = time.time() - file_start_time\n",
    "                print(f\"  - Error processing file pair {i}: {e} - Time: {file_time:.2f}s\")\n",
    "                \n",
    "                # Try to get file sizes if possible\n",
    "                try:\n",
    "                    file_size_high = os.path.getsize(main_folder + 'High/' + high_file)\n",
    "                    file_size_low = os.path.getsize(main_folder + 'Low/' + low_file)\n",
    "                except:\n",
    "                    file_size_high = 0\n",
    "                    file_size_low = 0\n",
    "                    \n",
    "                timing_stats.append({\n",
    "                    \"high_file\": high_file,\n",
    "                    \"low_file\": low_file,\n",
    "                    \"processing_time\": int(file_time * 1000),\n",
    "                    \"file_size_high\": file_size_high,\n",
    "                    \"file_size_low\": file_size_low,\n",
    "                    \"is_valid\": 0\n",
    "                })\n",
    "    \n",
    "    # Process timing stats\n",
    "    print(f\"\\nProcessing time summary:\")\n",
    "    total_valid = successful_files\n",
    "    total_files = len(high_freq_files)\n",
    "    \n",
    "    print(f\"Successfully processed {total_valid}/{total_files} file pairs\")\n",
    "    \n",
    "    # Sort by processing time to show slowest files\n",
    "    timing_stats.sort(key=lambda x: x[\"processing_time\"], reverse=True)\n",
    "    \n",
    "    print(f\"\\nTop 5 slowest files:\")\n",
    "    for i in range(min(5, len(timing_stats))):\n",
    "        row = timing_stats[i]\n",
    "        print(f\"  {row['high_file']} & {row['low_file']}: {row['processing_time']/1000:.2f}s \"\n",
    "              f\"({row['file_size_high']/1024:.1f}KB, {row['file_size_low']/1024:.1f}KB)\")\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    print(f\"\\nConverting to numpy arrays...\")\n",
    "    convert_start = time.time()\n",
    "    \n",
    "    Xs_arr = np.array(all_features)\n",
    "    ys_arr = np.array(all_y)\n",
    "    y4s_arr = np.array(all_y4)\n",
    "    y10s_arr = np.array(all_y10)\n",
    "    \n",
    "    convert_time = time.time() - convert_start\n",
    "    print(f\"Array conversion completed in {convert_time:.2f}s\")\n",
    "    \n",
    "    total_time = time.time() - start_time_total\n",
    "    print(f\"\\nTotal processing time: {total_time:.2f}s\")\n",
    "    print(f\"Final arrays - Xs: {Xs_arr.shape}, ys: {ys_arr.shape}, y4s: {y4s_arr.shape}, y10s: {y10s_arr.shape}\")\n",
    "    \n",
    "    return Xs_arr, ys_arr, y4s_arr, y10s_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a573bd8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started processing at 16:19:42\n",
      "Found 2 high frequency files and 2 low frequency files\n",
      "\n",
      "Processing batch 1/1 (files 1-2)\n",
      "Processing file 1/2: 10011H_0.csv (92069.7KB) & 10011L_0.csv (94588.4KB)\n",
      "  - Load time: 392.39s\n",
      "  - Processed 12 segments: Stack time: 0.05s, Split time: 0.00s, Total time: 392.45s\n",
      "Processing file 2/2: 11000H_0.csv (93690.4KB) & 11000L_0.csv (97027.5KB)\n",
      "  - Load time: 376.50s\n",
      "  - Processed 12 segments: Stack time: 0.05s, Split time: 0.00s, Total time: 376.54s\n",
      "\n",
      "Processing time summary:\n",
      "Successfully processed 2/2 file pairs\n",
      "\n",
      "Top 5 slowest files:\n",
      "  10011H_0.csv & 10011L_0.csv: 392.45s (92069.7KB, 94588.4KB)\n",
      "  11000H_0.csv & 11000L_0.csv: 376.54s (93690.4KB, 97027.5KB)\n",
      "\n",
      "Converting to numpy arrays...\n",
      "Array conversion completed in 0.08s\n",
      "\n",
      "Total processing time: 769.07s\n",
      "Final arrays - Xs: (24, 2, 800000), ys: (24,), y4s: (24,), y10s: (24,)\n",
      "length of X: 24 length of y: 24\n"
     ]
    }
   ],
   "source": [
    "# Dataset Info\n",
    "main_folder = './Data/DroneRF/'\n",
    "t_seg = 20\n",
    "Xs_arr, ys_arr, y4s_arr, y10s_arr = load_dronerf_raw(main_folder, t_seg)\n",
    "fs = 40e6 #40 MHz\n",
    "\n",
    "print('length of X:', len(Xs_arr), 'length of y:', len(ys_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4cde691",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_per_seg = 1024 # length of each segment (powers of 2)\n",
    "n_overlap_spec = 120\n",
    "win_type = 'hamming' # make ends of each segment match\n",
    "high_low = 'L' #'L', 'H' # high or low range of frequency\n",
    "feature_to_save = ['SPEC'] # what features to generate and save: SPEC or PSD\n",
    "format_to_save = ['IMG'] # IMG or ARR or RAW\n",
    "to_add = True\n",
    "spec_han_window = np.hanning(n_per_seg)\n",
    "\n",
    "# Image properties\n",
    "dim_px = (224, 224) # dimension of image pixels\n",
    "dpi = 100\n",
    "\n",
    "# Raw input len\n",
    "v_samp_len = 10000\n",
    "\n",
    "# data saving folders\n",
    "features_folder = dronerf_feat_path\n",
    "date_string = date.today()\n",
    "# folder naming: ARR_FEAT_NFFT_SAMPLELENGTH\n",
    "arr_spec_folder = \"ARR_SPEC_\"+high_low+'_'+str(n_per_seg)+\"_\"+str(t_seg)+\"/\"\n",
    "arr_psd_folder = \"ARR_PSD_\"+high_low+'_'+str(n_per_seg)+\"_\"+str(t_seg)+\"/\"\n",
    "img_spec_folder = \"IMG_SPEC_\"+high_low+'_'+str(n_per_seg)+\"_\"+str(t_seg)+\"/\"\n",
    "img_psd_folder = \"IMG_PSD_\"+high_low+'_'+str(n_per_seg)+\"_\"+str(t_seg)+\"/\"\n",
    "raw_folder = 'RAW_VOLT_'+str(v_samp_len)+\"_\"+str(t_seg)+\"/\" # high and low frequency stacked together\n",
    "\n",
    "existing_folders = os.listdir(features_folder)\n",
    "\n",
    "if high_low == 'H':\n",
    "    i_hl = 0\n",
    "elif high_low == 'L':\n",
    "    i_hl = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5abd7a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating SPEC in IMAGE format\n"
     ]
    }
   ],
   "source": [
    "# check if this set of parameters already exists\n",
    "# check if each of the 4 folders exist\n",
    "sa_save = False   #spec array\n",
    "si_save = False   #spec imag\n",
    "pa_save = False   #psd array\n",
    "pi_save = False   #psd imag\n",
    "raw_save = False # raw high low signals\n",
    "\n",
    "if 'SPEC' in feature_to_save:\n",
    "    if 'ARR' in format_to_save:\n",
    "        if arr_spec_folder not in existing_folders or to_add:\n",
    "            try:\n",
    "                os.mkdir(features_folder+arr_spec_folder)\n",
    "            except:\n",
    "                print('folder already exist - adding')\n",
    "            sa_save = True\n",
    "            print('Generating SPEC in ARRAY format')\n",
    "        else:\n",
    "            print('Spec Arr folder already exists')\n",
    "    if 'IMG' in format_to_save:\n",
    "        if img_spec_folder not in existing_folders or to_add:\n",
    "            try:\n",
    "                os.mkdir(features_folder+img_spec_folder)\n",
    "            except:\n",
    "                print('folder already exist - adding')\n",
    "            si_save = True\n",
    "            print('Generating SPEC in IMAGE format')\n",
    "        else:\n",
    "            print('Spec Arr folder already exists')\n",
    "if 'PSD' in feature_to_save:\n",
    "    if 'ARR' in format_to_save:\n",
    "        if arr_psd_folder not in existing_folders or to_add:\n",
    "            try:\n",
    "                os.mkdir(features_folder+arr_psd_folder)\n",
    "            except:\n",
    "                print('folder already exist - adding')\n",
    "            pa_save = True\n",
    "            print('Generating PSD in ARRAY format')\n",
    "        else:\n",
    "            print('PSD Arr folder already exists')\n",
    "    if 'IMG' in format_to_save:\n",
    "        if img_psd_folder not in existing_folders or to_add:\n",
    "            try:\n",
    "                os.mkdir(features_folder+img_psd_folder)\n",
    "            except:\n",
    "                print('folder already exist - adding')\n",
    "            pi_save = True\n",
    "            print('Generating PSD in IMAGE format')\n",
    "        else:\n",
    "            print('PSD Arr folder already exists')\n",
    "\n",
    "if 'RAW' in feature_to_save:\n",
    "    if raw_folder in existing_folders or to_add:\n",
    "        try:\n",
    "            os.mkdir(features_folder+raw_folder)\n",
    "        except:\n",
    "            print('RAW V folder already exists')\n",
    "        raw_save = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd991c34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.94it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  9.56it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13.59it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13.33it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11.30it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11.18it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13.00it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12.33it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11.48it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12.41it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11.49it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12.05it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10.93it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11.17it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11.48it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11.69it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11.04it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11.97it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11.97it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12.04it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12.77it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10.86it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11.56it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if all([not sa_save, not si_save, not pa_save, not pi_save, not raw_save]):\n",
    "    print('Features Already Exist - Do Not Generate')\n",
    "else:\n",
    "    n_parts = 24 # process the data in 10 parts so memory doesn't overwhelm\n",
    "\n",
    "    indices_ranges = np.split(np.array(range(len(Xs_arr))), n_parts) \n",
    "    for i in range(n_parts):\n",
    "        BILABEL = []\n",
    "        DRONELABEL = []\n",
    "        MODELALBEL = []\n",
    "        F_PSD = []\n",
    "        F_SPEC = []\n",
    "        F_V = []\n",
    "        ir = indices_ranges[i]\n",
    "        for j in tqdm(range(len(ir))):\n",
    "            d_real = Xs_arr[ir[j]][i_hl]\n",
    "            \n",
    "            # if save raw data\n",
    "            if raw_save:\n",
    "                t = np.arange(0, len(d_real))\n",
    "                f_high = interpolate.interp1d(t, Xs_arr[ir[j]][0])\n",
    "                f_low = interpolate.interp1d(t, Xs_arr[ir[j]][1])\n",
    "                tt = np.linspace(0, len(d_real)-1, num=v_samp_len)\n",
    "\n",
    "                d_v = np.stack((f_high(tt), f_low(tt)), axis=0)\n",
    "                F_V.append(d_v)\n",
    "            \n",
    "            if pa_save or pi_save:\n",
    "            # calculate PSD\n",
    "                fpsd, Pxx_den = signal.welch(d_real, fs, window=win_type, nperseg=n_per_seg)\n",
    "                if pa_save:\n",
    "                    F_PSD.append(Pxx_den)\n",
    "                if pi_save:\n",
    "                    save_psd_image_rf(features_folder, img_psd_folder,\n",
    "                                      y10s_arr[ir[j]], i, j, Pxx_den, dim_px, dpi)\n",
    "            \n",
    "            if sa_save or si_save:\n",
    "            # calculate spectrogram\n",
    "            # welch's method older\n",
    "#           fspec, t, Sxx = signal.spectrogram(d_real, fs, window=win_type, nperseg=n_per_seg)\n",
    "            \n",
    "                if si_save: # set up fig properties if saving images\n",
    "                    plt.clf()\n",
    "                    fig,ax = plt.subplots(1, figsize=(dim_px[0]/dpi, dim_px[1]/dpi), dpi=dpi)\n",
    "                    fig.subplots_adjust(left=0,right=1,bottom=0,top=1)\n",
    "                    ax.axis('tight')\n",
    "                    ax.axis('off')\n",
    "\n",
    "                spec, _, _, _ = plt.specgram(d_real, NFFT=n_per_seg, Fs=fs, window=spec_han_window, \n",
    "                                  noverlap=n_overlap_spec, sides='onesided')\n",
    "                if si_save:\n",
    "                    save_spec_image_fig_rf(features_folder, img_spec_folder, \n",
    "                                           y10s_arr[ir[j]], i, j, fig, dpi)\n",
    "                if sa_save:\n",
    "                    F_SPEC.append(interpolate_2d(Sxx, (224,224)))\n",
    "\n",
    "            # Labels\n",
    "            BILABEL.append(ys_arr[ir[j]])\n",
    "            DRONELABEL.append(y4s_arr[ir[j]])\n",
    "            MODELALBEL.append(y10s_arr[ir[j]])\n",
    "        \n",
    "        if sa_save:\n",
    "            save_array_rf(features_folder+arr_spec_folder, F_SPEC, BILABEL, DRONELABEL, MODELALBEL, 'SPEC', n_per_seg, i)\n",
    "        if pa_save:\n",
    "            save_array_rf(features_folder+arr_psd_folder, F_PSD, BILABEL, DRONELABEL, MODELALBEL, 'PSD', n_per_seg, i)\n",
    "        if raw_save:\n",
    "            save_array_rf(features_folder+raw_folder, F_V, BILABEL, DRONELABEL, MODELALBEL, 'RAW', '', i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7df7fd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbd07254",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(not a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7721a057",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gddrone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
